{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import logging\n",
    "\n",
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Initialize logger\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# Define the Word2Vec transformer class\n",
    "class W2V(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, num_words=None, **kwargs):\n",
    "        self.num_words = num_words\n",
    "        self.tokenizer = Tokenizer(num_words=num_words, **kwargs)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.Word2 = api.load(\"word2vec-google-news-300\")\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        x = np.array(X.values)\n",
    "        for i in range(len(x)):\n",
    "            for j in range(len(x[i])):\n",
    "                tokens = x[i][j].split()\n",
    "                embeddings = [\n",
    "                    self.Word2[token]\n",
    "                    for token in tokens\n",
    "                    if token in self.Word2.key_to_index\n",
    "                ]\n",
    "                if len(embeddings) > 0:\n",
    "                    mean = np.mean(embeddings)\n",
    "                else:\n",
    "                    mean = 0\n",
    "                x[i][j] = mean\n",
    "        return x\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {\"num_words\": self.num_words}\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "X_train_file = \"../data/X_train.csv\"\n",
    "y_train_file = \"../data/Y_train.csv\"\n",
    "\n",
    "with open(X_train_file, \"r\") as f:\n",
    "    mixed_columns = (\n",
    "        [\"item\" + str(i) for i in range(1, 25)]\n",
    "        + [\"make\" + str(i) for i in range(1, 25)]\n",
    "        + [\"model\" + str(i) for i in range(1, 25)]\n",
    "        + [\"goods_code\" + str(i) for i in range(1, 25)]\n",
    "    )\n",
    "    mixed_columns_dtype = {col: str for col in mixed_columns}\n",
    "    X_train_df = pd.read_csv(X_train_file, dtype=mixed_columns_dtype)\n",
    "\n",
    "with open(y_train_file, \"r\") as f:\n",
    "    y_train_df = pd.read_csv(f)\n",
    "\n",
    "cols_base = [\"goods_code\"]\n",
    "columns_to_drop = [\"ID\"] + [col + str(i) for col in cols_base for i in range(1, 25)]\n",
    "\n",
    "X_train_df = X_train_df.drop(columns_to_drop, axis=1)\n",
    "y_train_df = y_train_df[\"fraud_flag\"]\n",
    "\n",
    "# Identify the columns to apply RNN tokenization\n",
    "rnn_columns = [\"make\", \"item\", \"model\"]  # Add more columns as needed\n",
    "rnn_columns = [col + str(i) for col in rnn_columns for i in range(1, 25)]\n",
    "\n",
    "# Identify the categorical and numerical columns\n",
    "categorical_columns = rnn_columns\n",
    "numerical_columns = [\n",
    "    col for col in X_train_df.columns if col not in categorical_columns\n",
    "]\n",
    "\n",
    "# Clean data\n",
    "for col in categorical_columns:\n",
    "    X_train_df[col] = X_train_df[col].fillna(\"\")\n",
    "for col in numerical_columns:\n",
    "    X_train_df[col] = X_train_df[col].fillna(0)\n",
    "\n",
    "# Define transformers\n",
    "cat_pipeline = make_pipeline(W2V())\n",
    "num_pipeline = make_pipeline(StandardScaler())\n",
    "\n",
    "# Create the preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat_pipeline\", cat_pipeline, categorical_columns),\n",
    "        (\"num_pipeline\", num_pipeline, numerical_columns),\n",
    "    ]\n",
    ")\n",
    "\n",
    "rfc = RandomForestClassifier(random_state=42, verbose=True)\n",
    "pipeline = make_pipeline((\"preprocessor\", preprocessor), (\"rfc\", rfc), verbose=True)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_df, y_train_df, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "param_grid = {\n",
    "    \"n_estimators\": [100, 500],\n",
    "    # \"max_features\": [\"auto\", \"sqrt\", \"log2\"],\n",
    "    \"max_depth\": [None, 13, 20, 50],\n",
    "    # \"criterion\": [\"gini\", \"entropy\"],\n",
    "    # \"class_weight\":[{1: 100}, None]\n",
    "}\n",
    "\n",
    "# Define models and their respective hyperparameters\n",
    "grid = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    scoring=\"average_precision\",\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Fit the pipeline\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the pipeline using average_precision_score\n",
    "y_pred = pipeline.predict_proba(X_val)\n",
    "average_precision = average_precision_score(y_val, y_pred[:, 1]) * 100\n",
    "logger.info(f\"Average precision score: {average_precision}\")\n",
    "\n",
    "# Save the trained model\n",
    "import joblib\n",
    "\n",
    "model_filename = \"trained_rf_classifier.pkl\"\n",
    "joblib.dump(pipeline, model_filename)\n",
    "logger.info(f\"Trained model saved as {model_filename}\")\n",
    "\n",
    "# Load and use the trained model for predictions\n",
    "loaded_pipeline = joblib.load(model_filename)\n",
    "sample_input = X_val.iloc[:5, :]  # Take a sample input for prediction\n",
    "sample_output = loaded_pipeline.predict_proba(sample_input)\n",
    "logger.info(f\"Sample input predictions: {sample_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import logging\n",
    "\n",
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Initialize logger\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# Define the Word2Vec transformer class\n",
    "class W2V(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, num_words=None, **kwargs):\n",
    "        self.num_words = num_words\n",
    "        self.tokenizer = Tokenizer(num_words=num_words, **kwargs)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.Word2 = api.load(\"word2vec-google-news-300\")\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        x = np.array(X.values)\n",
    "        for i in range(len(x)):\n",
    "            for j in range(len(x[i])):\n",
    "                tokens = x[i][j].split()\n",
    "                embeddings = [\n",
    "                    self.Word2[token]\n",
    "                    for token in tokens\n",
    "                    if token in self.Word2.key_to_index\n",
    "                ]\n",
    "                if len(embeddings) > 0:\n",
    "                    mean = np.mean(embeddings)\n",
    "                else:\n",
    "                    mean = 0\n",
    "                x[i][j] = mean\n",
    "        return x\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {\"num_words\": self.num_words}\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "X_train_file = \"../data/X_train.csv\"\n",
    "y_train_file = \"../data/Y_train.csv\"\n",
    "\n",
    "with open(X_train_file, \"r\") as f:\n",
    "    mixed_columns = (\n",
    "        [\"item\" + str(i) for i in range(1, 25)]\n",
    "        + [\"make\" + str(i) for i in range(1, 25)]\n",
    "        + [\"model\" + str(i) for i in range(1, 25)]\n",
    "        + [\"goods_code\" + str(i) for i in range(1, 25)]\n",
    "    )\n",
    "    mixed_columns_dtype = {col: str for col in mixed_columns}\n",
    "    X_train_df = pd.read_csv(X_train_file, dtype=mixed_columns_dtype)\n",
    "\n",
    "with open(y_train_file, \"r\") as f:\n",
    "    y_train_df = pd.read_csv(f)\n",
    "\n",
    "cols_base = [\"goods_code\"]\n",
    "columns_to_drop = [\"ID\"] + [col + str(i) for col in cols_base for i in range(1, 25)]\n",
    "\n",
    "X_train_df = X_train_df.drop(columns_to_drop, axis=1)\n",
    "y_train_df = y_train_df[\"fraud_flag\"]\n",
    "\n",
    "# Identify the columns to apply RNN tokenization\n",
    "rnn_columns = [\"make\", \"item\", \"model\"]  # Add more columns as needed\n",
    "rnn_columns = [col + str(i) for col in rnn_columns for i in range(1, 25)]\n",
    "\n",
    "# Identify the categorical and numerical columns\n",
    "categorical_columns = rnn_columns\n",
    "numerical_columns = [\n",
    "    col for col in X_train_df.columns if col not in categorical_columns\n",
    "]\n",
    "\n",
    "# Clean data\n",
    "for col in categorical_columns:\n",
    "    X_train_df[col] = X_train_df[col].fillna(\"\")\n",
    "for col in numerical_columns:\n",
    "    X_train_df[col] = X_train_df[col].fillna(0)\n",
    "\n",
    "# Define transformers\n",
    "cat_pipeline = make_pipeline(W2V())\n",
    "num_pipeline = make_pipeline(StandardScaler())\n",
    "\n",
    "# Create the preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat_pipeline\", cat_pipeline, categorical_columns),\n",
    "        (\"num_pipeline\", num_pipeline, numerical_columns),\n",
    "    ]\n",
    ")\n",
    "\n",
    "param_grid = {\n",
    "    \"n_estimators\": [100, 500],\n",
    "    # \"max_features\": [\"auto\", \"sqrt\", \"log2\"],\n",
    "    \"max_depth\": [None, 13, 20, 50],\n",
    "    # \"criterion\": [\"gini\", \"entropy\"],\n",
    "    # \"class_weight\":[{1: 100}, None]\n",
    "}\n",
    "\n",
    "rfc = RandomForestClassifier(random_state=42, verbose=True)\n",
    "# Define models and their respective hyperparameters\n",
    "grid = GridSearchCV(estimator=rfc, scoring=\"average_precision\", param_grid=param_grid, cv=5, n_jobs=-1, verbose=True)\n",
    "\n",
    "pipeline = make_pipeline(preprocessor, grid, verbose=True)\n",
    "# Create the pipeline\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_df, y_train_df, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Fit the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the pipeline using average_precision_score\n",
    "y_pred = pipeline.predict_proba(X_val)\n",
    "average_precision = average_precision_score(y_val, y_pred[:, 1]) * 100\n",
    "logger.info(f\"Average precision score: {average_precision}\")\n",
    "\n",
    "# Save the trained model\n",
    "import joblib\n",
    "\n",
    "model_filename = \"trained_rf_classifier.pkl\"\n",
    "joblib.dump(pipeline, model_filename)\n",
    "logger.info(f\"Trained model saved as {model_filename}\")\n",
    "\n",
    "# Load and use the trained model for predictions\n",
    "loaded_pipeline = joblib.load(model_filename)\n",
    "sample_input = X_val.iloc[:5, :]  # Take a sample input for prediction\n",
    "sample_output = loaded_pipeline.predict_proba(sample_input)\n",
    "logger.info(f\"Sample input predictions: {sample_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tranformed = preprocessor.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"n_estimators\": [100, 500, 1000],\n",
    "    \"max_features\": [\"auto\", \"sqrt\", \"log2\"],\n",
    "    \"max_depth\": [None, 13, 20, 50],\n",
    "    # \"criterion\": [\"gini\", \"entropy\"],\n",
    "    \"class_weight\":[{1: 100}, None]\n",
    "}\n",
    "\n",
    "rfc = RandomForestClassifier(random_state=42, verbose=True)\n",
    "# Define models and their respective hyperparameters\n",
    "grid = GridSearchCV(\n",
    "    estimator=rfc,\n",
    "    scoring=\"average_precision\",\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Fit the pipeline\n",
    "grid.fit(X_train_tranformed, y_train)\n",
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tranformed = preprocessor.transform(X_train_df)\n",
    "grid.best_estimator_.fit(X_tranformed, y_train_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_file = \"../data/X_test.csv\"\n",
    "\n",
    "with open(X_test_file, \"r\") as f:\n",
    "    mixed_columns = (\n",
    "        [\"item\" + str(i) for i in range(1, 25)]\n",
    "        + [\"make\" + str(i) for i in range(1, 25)]\n",
    "        + [\"model\" + str(i) for i in range(1, 25)]\n",
    "        + [\"goods_code\" + str(i) for i in range(1, 25)]\n",
    "    )\n",
    "    mixed_columns_dtype = {col: str for col in mixed_columns}\n",
    "    X_test_df = pd.read_csv(X_test_file, dtype=mixed_columns_dtype)\n",
    "\n",
    "X_test_df = X_test_df.drop(columns_to_drop, axis=1)\n",
    "\n",
    "# Identify the categorical and numerical columns\n",
    "categorical_columns = rnn_columns\n",
    "numerical_columns = [col for col in X_test_df.columns if col not in categorical_columns]\n",
    "\n",
    "# Clean data\n",
    "for col in categorical_columns:\n",
    "    X_test_df[col] = X_test_df[col].fillna(\"\")\n",
    "for col in numerical_columns:\n",
    "    X_test_df[col] = X_test_df[col].fillna(0)\n",
    "\n",
    "X_test_tranformed = preprocessor.transform(X_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = grid.best_estimator_.predict_proba(X_test_tranformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_ = out[:, 1]\n",
    "with open(X_test_file, \"r\") as f:\n",
    "    X_test_df = pd.read_csv(X_test_file, dtype=mixed_columns_dtype)\n",
    "    df = pd.DataFrame({\"ID\": X_test_df[\"ID\"], \"fraud_flag\": out_})\n",
    "    df.to_csv(\"out.csv\")\n",
    "    df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"fraud_flag\"].hist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Challenge0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
