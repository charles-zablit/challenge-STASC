{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=center>\n",
    "<h1>Comment démasquer les fraudeurs ?</h1>\n",
    "<h2>par BNP Paribas PF</h2>\n",
    "<h3>MICHELON François</h3>\n",
    "<h3>ZABLIT Charles</h3>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Configuration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Imports de modules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pathlib\n",
    "import re\n",
    "from typing import Tuple, List\n",
    "from collections import Counter\n",
    "\n",
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    train_test_split,\n",
    "    StratifiedShuffleSplit,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import scale, StandardScaler\n",
    "import sklearn.metrics as metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 Configuration du logger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize logger\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.3 Configuration des chemins de fichiers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_file = pathlib.Path(\"data/X_train_G3tdtEn.csv\")\n",
    "y_train_file = pathlib.Path(\"data/Y_train_2_XPXJDyy.csv\")\n",
    "X_test_file = pathlib.Path(\"data/X_test_8skS2ey.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Analyse des données\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Chargement des données\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_df(path: pathlib.Path) -> pd.DataFrame:\n",
    "    mixed_columns = (\n",
    "        [\"item\" + str(i) for i in range(1, 25)]\n",
    "        + [\"make\" + str(i) for i in range(1, 25)]\n",
    "        + [\"model\" + str(i) for i in range(1, 25)]\n",
    "        + [\"goods_code\" + str(i) for i in range(1, 25)]\n",
    "    )\n",
    "    dtype = {col: str for col in mixed_columns}\n",
    "    return pd.read_csv(path, dtype=dtype).drop(columns=[\"ID\"])\n",
    "\n",
    "\n",
    "def load_test_df(path: pathlib.Path) -> pd.Series:\n",
    "    return pd.read_csv(path)[\"fraud_flag\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df = load_train_df(X_train_file)\n",
    "y_train_df = load_test_df(y_train_file)\n",
    "X_test_df = load_train_df(X_test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Étude de la parcimonie des données\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_coeff(X: pd.DataFrame) -> float:\n",
    "    nb_sparse = X_train_df.isnull().sum().sum()\n",
    "    nb_total = X_train_df.shape[0] * X_train_df.shape[1]\n",
    "    return nb_sparse / nb_total\n",
    "\n",
    "\n",
    "print(f\"{sparse_coeff(X_train_df):.2%} des données sont nulles.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df.notnull().sum()[[f\"item{i}\" for i in range(1, 25)]].plot.pie(\n",
    "    autopct=\"%1.0f%%\",\n",
    ")\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Répartition des items non nuls\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "92% des commandes ont moins de 3 items.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Étude des tendances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df[y_train_df == 0].notnull().sum()[\n",
    "    [f\"item{i}\" for i in range(5, 25)]\n",
    "].plot.bar()\n",
    "plt.ylabel(\"Nombre de valeurs non nulles\")\n",
    "plt.xlabel(\"Colonne\")\n",
    "plt.xticks(rotation=45);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df[y_train_df == 1].notnull().sum()[\n",
    "    [f\"item{i}\" for i in range(5, 25)]\n",
    "].plot.bar()\n",
    "plt.ylabel(\"Nombre de valeurs non nulles\")\n",
    "plt.xlabel(\"Colonne\")\n",
    "plt.xticks(rotation=45);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En regardant pour les colonnes items > 5, fraude+non fraude et fraude, on n'observe pas de tendance sur le nombre d'items par panier entre les commandes frauduleuses et les non frauduleuses.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df[(y_train_df == 0) & (X_train_df[\"Nb_of_items\"] < 25)][\n",
    "    \"Nb_of_items\"\n",
    "].plot.hist(bins=25)\n",
    "plt.ylabel(\"Nombre de commandes\")\n",
    "plt.xlabel(\"Number_of_items\")\n",
    "plt.xticks(rotation=45);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df[y_train_df == 1][\"Nb_of_items\"].plot.hist(bins=25)\n",
    "plt.ylabel(\"Nombre de commandes\")\n",
    "plt.xlabel(\"Number_of_items\")\n",
    "plt.xticks(rotation=45);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Là encore, on ne distingue pas de tendance significatives sur la distribution du nombre d'items par panier.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_price(X: pd.DataFrame) -> pd.Series:\n",
    "    return X[[f\"cash_price{i}\" for i in range(1, 25)]].sum(axis=1)\n",
    "\n",
    "\n",
    "def plot_total_price_hist(X: pd.DataFrame, y: pd.DataFrame, fraud: bool) -> None:\n",
    "    if fraud:\n",
    "        total_price_df = total_price(X[(total_price(X) < 10000) & (y == 1)])\n",
    "    else:\n",
    "        total_price_df = total_price(X[(total_price(X) < 10000) & (y == 0)])\n",
    "    total_price_df.plot.hist(bins=25)\n",
    "    mean = total_price_df.mean()\n",
    "    std = total_price_df.std()\n",
    "    plt.xlim(0, 10000)\n",
    "    plt.axvline(\n",
    "        x=mean,\n",
    "        color=\"red\",\n",
    "    )\n",
    "    plt.text(mean, 0, f\"Moyenne = {mean:.0f}\", rotation=90)\n",
    "    plt.axvline(\n",
    "        x=std,\n",
    "        color=\"green\",\n",
    "    )\n",
    "    plt.text(std, 0, f\"Ecart type = {std:.0f}\", rotation=90)\n",
    "    plt.ylabel(\"Nombre de commandes\")\n",
    "    plt.xlabel(\"Prix total de la commande\")\n",
    "\n",
    "\n",
    "plot_total_price_hist(X_train_df, y_train_df, fraud=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_total_price_hist(X_train_df, y_train_df, fraud=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On observe une légère augmentation de la moyenne et de l'écart type du prix total des commandes pour les fraudes. Nous allons essayer d'utiliser ces informations dans nos features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Étude de la distribution des classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_df.value_counts().plot.pie(\n",
    "    autopct=\"%1.0f%%\",\n",
    ")\n",
    "plt.legend([\"Non frauduleux\", \"Frauduleux\"])\n",
    "plt.ylabel(\"\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C'est un jeux de données de détection de fraudes, donc la distribution frauduleux/non frauduleux est très inégale.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Feature engineering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Création de nouvelles features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeaturesExtractor(BaseEstimator, TransformerMixin):\n",
    "    _counter: Counter\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        all_goods = []\n",
    "        for _, row in X.iterrows():\n",
    "            for i in range(1, 25):\n",
    "                all_goods.append(row[f\"goods_code{i}\"])\n",
    "        self._counter = Counter(all_goods)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: pd.DataFrame, y=None):\n",
    "        df = X\n",
    "        # Create a column with the total price of the purchase.\n",
    "        df[\"total_price\"] = df[[f\"cash_price{i}\" for i in range(1, 25)]].sum(axis=1)\n",
    "        # Create a column with the average price of the items in the purchase.\n",
    "        df[\"avg_item_price\"] = df[\"total_price\"] / df[\"Nb_of_items\"]\n",
    "        # Create a column with the maximum price of the items in the purchase.\n",
    "        df[\"max_item_price\"] = df[[f\"cash_price{i}\" for i in range(1, 25)]].max(axis=1)\n",
    "        # Create a column with the minimum price of the items in the purchase.\n",
    "        df[\"min_item_price\"] = df[[f\"cash_price{i}\" for i in range(1, 25)]].min(axis=1)\n",
    "        # Create a column with the variance of the prices of the items in the purchase.\n",
    "        df[\"price_variance\"] = df[[f\"cash_price{i}\" for i in range(1, 25)]].var(axis=1)\n",
    "        # Create a column with the number of products in the purchase.\n",
    "        df[\"product_count\"] = df[[f\"Nbr_of_prod_purchas{i}\" for i in range(1, 25)]].sum(\n",
    "            axis=1\n",
    "        )\n",
    "        # Create a column with the number of unique manufacturers in the purchase.\n",
    "        df[\"nb_unique_makes\"] = df[[f\"make{i}\" for i in range(1, 25)]].nunique(axis=1)\n",
    "        # Create a column with the number of unique items in the purchase.\n",
    "        df[\"nb_unique_models\"] = df[[f\"model{i}\" for i in range(1, 25)]].nunique(axis=1)\n",
    "        # Create a column of the ratio of items to products in the purchase.\n",
    "        df[\"item_to_product_ratio\"] = df[\"Nb_of_items\"] / df[\"product_count\"]\n",
    "        # Create a column of the most common item in the purchase.\n",
    "        df[\"most_common_item\"] = df[\n",
    "            [f\"Nbr_of_prod_purchas{i}\" for i in range(1, 25)]\n",
    "        ].idxmax(axis=1)\n",
    "        most_common_item = []\n",
    "        for _, row in df.iterrows():\n",
    "            match = re.search(r\"(\\d+)\", row[\"most_common_item\"])\n",
    "            item_number = int(match.group(1))\n",
    "            most_common_item.append(row[f\"item{item_number}\"])\n",
    "        df[\"most_common_item\"] = most_common_item\n",
    "        df[\"most_common_item\"] = df[\"most_common_item\"].astype(str)\n",
    "\n",
    "        # For each goods_code, count the number of times it appears in the dataset.\n",
    "        for index, row in df.iterrows():\n",
    "            for i in range(1, 25):\n",
    "                if isinstance(row[f\"goods_code{i}\"], str):\n",
    "                    df.at[index, f\"goods_code{i}\"] = self._counter[\n",
    "                        row[f\"goods_code{i}\"]\n",
    "                    ]\n",
    "                else:\n",
    "                    df.at[index, f\"goods_code{i}\"] = 0\n",
    "        df[[\"goods_code\" + str(i) for i in range(1, 25)]] = df[\n",
    "            [\"goods_code\" + str(i) for i in range(1, 25)]\n",
    "        ].astype(int)\n",
    "\n",
    "        # Identify the columns to apply RNN tokenization\n",
    "        categorical_columns = [\"make\", \"item\", \"model\"]  # Add more columns as needed\n",
    "        categorical_columns = [\n",
    "            col + str(i) for col in categorical_columns for i in range(1, 25)\n",
    "        ]\n",
    "        categorical_columns.append(\"most_common_item\")\n",
    "\n",
    "        # Identify the categorical and numerical columns\n",
    "        numerical_columns = [\n",
    "            col for col in df.columns if col not in set(categorical_columns)\n",
    "        ]\n",
    "\n",
    "        # Clean data\n",
    "        for col in categorical_columns:\n",
    "            df[col] = df[col].fillna(\"\")\n",
    "        for col in numerical_columns:\n",
    "            df[col] = df[col].fillna(0)\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Transformer Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, num_words=None, **kwargs):\n",
    "        self.num_words = num_words\n",
    "        self.tokenizer = Tokenizer(num_words=num_words, **kwargs)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.Word2 = api.load(\"word2vec-google-news-300\")\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        x = np.array(X.values)\n",
    "        for i in range(len(x)):\n",
    "            for j in range(len(x[i])):\n",
    "                tokens = x[i][j].split()\n",
    "                embeddings = [\n",
    "                    self.Word2[token]\n",
    "                    for token in tokens\n",
    "                    if token in self.Word2.key_to_index\n",
    "                ]\n",
    "                if len(embeddings) > 0:\n",
    "                    mean = np.mean(embeddings)\n",
    "                else:\n",
    "                    mean = 0\n",
    "                x[i][j] = mean\n",
    "        return x\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {\"num_words\": self.num_words}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Définition de la pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pipeline() -> Pipeline:\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"text_pipeline\", Word2Vec(), make_column_selector(dtype_include=object)),\n",
    "            (\"num_pipeline\", StandardScaler(), make_column_selector(dtype_include=np.number)),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return Pipeline(\n",
    "        steps=[\n",
    "            (\"features_extract\", FeaturesExtractor()),\n",
    "            (\"preprocess_static\", preprocessor),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Étude de la corrélation des données\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_corr(X: np.ndarray, y: pd.Series) -> pd.DataFrame:\n",
    "    columns = (\n",
    "        [f\"item{i}\" for i in range(1, 25)]\n",
    "        + [f\"cash_price{i}\" for i in range(1, 25)]\n",
    "        + [f\"make{i}\" for i in range(1, 25)]\n",
    "        + [f\"model{i}\" for i in range(1, 25)]\n",
    "        + [f\"goods_code{i}\" for i in range(1, 25)]\n",
    "        + [f\"Nbr_of_prod_purchas{i}\" for i in range(1, 25)]\n",
    "        + [\n",
    "            \"Nb_of_items\",\n",
    "            \"total_price\",\n",
    "            \"avg_item_price\",\n",
    "            \"max_item_price\",\n",
    "            \"min_item_price\",\n",
    "            \"price_variance\",\n",
    "            \"product_count\",\n",
    "            \"nb_unique_makes\",\n",
    "            \"nb_unique_models\",\n",
    "            \"item_to_product_ratio\",\n",
    "            \"most_common_item\",\n",
    "        ]\n",
    "    )\n",
    "    df = pd.DataFrame(\n",
    "        X,\n",
    "        columns=columns,\n",
    "    )\n",
    "    df[\"fraud_flag\"] = y\n",
    "    return df.corr()[y.name].abs().sort_values(ascending=False)\n",
    "\n",
    "\n",
    "label_corr(\n",
    "    generate_pipeline().fit_transform(X_train_df, y_train_df),\n",
    "    y_train_df,\n",
    ").head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Représentation TSNE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tsne(X: pd.DataFrame, y: pd.Series) -> None:\n",
    "    X = generate_pipeline().fit_transform(X, y)\n",
    "    data_norm = scale(X)\n",
    "    data_tsne = TSNE().fit_transform(data_norm)\n",
    "    data_tsne_df = pd.DataFrame(\n",
    "        {\n",
    "            \"Dim1\": data_tsne[:, 0],\n",
    "            \"Dim2\": data_tsne[:, 1],\n",
    "            \"target\": y,\n",
    "        }\n",
    "    )\n",
    "    sns.scatterplot(data_tsne_df, x=\"Dim1\", y=\"Dim2\", hue=\"target\")\n",
    "\n",
    "\n",
    "plot_tsne(X_train_df, y_train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La représentation TSNE et les faibles valeurs de corrélation linéaire données précédemment nous poussent à choisir un classifieur par arbre de décision comme le `RandomForest`, plutôt que des classifieurs linéaires."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Entraînement du modèle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Tuning des hyperparamètres\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(\n",
    "    Pipeline(\n",
    "        steps=[\n",
    "            (\"preprocessor\", generate_pipeline()),\n",
    "            (\"clf\", RandomForestClassifier(random_state=0)),\n",
    "        ]\n",
    "    ),\n",
    "    # param_grid={\n",
    "    #     \"clf__bootstrap\": [True],\n",
    "    #     \"clf__n_estimators\": [395, 400, 405],\n",
    "    #     \"clf__max_depth\": [19, 20, 21],\n",
    "    #     \"clf__min_samples_split\": [2, 3, 5, 7],\n",
    "    #     \"clf__min_samples_leaf\": [1, 2, 3],\n",
    "    #     \"clf__class_weight\": [None, \"balanced\"],\n",
    "    # },\n",
    "    \n",
    "    # Pour réduire le temps d'éxecution, on ne recalcule pas les paramètres optimaux.\n",
    "    param_grid={\n",
    "        \"clf__bootstrap\": [True],\n",
    "        \"clf__n_estimators\": [400],\n",
    "        \"clf__max_depth\": [20],\n",
    "        \"clf__min_samples_split\": [5],\n",
    "        \"clf__min_samples_leaf\": [1],\n",
    "        \"clf__class_weight\": [None],\n",
    "    },\n",
    "    scoring=\"average_precision\",\n",
    "    cv=StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=0),\n",
    "    n_jobs=-1,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Split des données\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_train_df, y_train_df, test_size=0.2, stratify=y_train_df, random_state=0\n",
    ")\n",
    "print(f\"{y_train.value_counts()[1] / len(y_train):.5%} de fraudes dans le train set.\")\n",
    "print(f\"{y_test.value_counts()[1] / len(y_test):.5%} de fraudes dans le test set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Entraînement du modèle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {k.replace(\"clf__\", \"\"):v for k,v in grid.best_params_.items()}\n",
    "print(f\"Meilleurs paramètres du GridSearch: {best_params}\")\n",
    "print(f\"Meilleur score du GridSearch: {grid.best_score_:0.5%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Pipeline finale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pipeline = make_pipeline(\n",
    "    generate_pipeline(),\n",
    "    RandomForestClassifier(random_state=0, n_jobs=-1, **best_params),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Évaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_train_df, y_train_df, test_size=0.2, stratify=y_train_df, random_state=0\n",
    ")\n",
    "rf_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.PrecisionRecallDisplay.from_estimator(\n",
    "    rf_pipeline, X_test, y_test, name=\"RandomForest\", plot_chance_level=True\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Prédiction sur les données de test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_test_predictions(\n",
    "    X_train_df: pd.DataFrame,\n",
    "    y_train_df: pd.Series,\n",
    "    X_test_df: pd.DataFrame,\n",
    "    rf_pipeline: Pipeline,\n",
    ") -> pd.DataFrame:\n",
    "    rf_pipeline.fit(X_train_df, y_train_df)\n",
    "    out = rf_pipeline.predict_proba(X_test_df)\n",
    "    out_ = out[:, 1]\n",
    "    IDs = pd.read_csv(X_test_file)[\"ID\"]\n",
    "    df = pd.DataFrame({\"ID\": IDs, \"fraud_flag\": out_})\n",
    "    df = df.reset_index()\n",
    "    df.to_csv(\"out.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Challenge0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
